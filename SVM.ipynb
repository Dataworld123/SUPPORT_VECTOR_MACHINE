{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "63781c0b-2985-4a27-bdc8-32337a35bb67",
   "metadata": {},
   "source": [
    "Q1. What is the mathematical formula for a linear SVM?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8101a16-550a-4f89-a662-43d75db17427",
   "metadata": {},
   "source": [
    "A linear Support Vector Machine (SVM) aims to find a hyperplane that best separates the data into different classes. The mathematical formula for a linear SVM can be expressed as follows:\n",
    "\n",
    "Given a set of training data \n",
    "(\n",
    "�\n",
    "�\n",
    ",\n",
    "�\n",
    "�\n",
    ")\n",
    "(x \n",
    "i\n",
    "​\n",
    " ,y \n",
    "i\n",
    "​\n",
    " ), where \n",
    "�\n",
    "�\n",
    "x \n",
    "i\n",
    "​\n",
    "  is a feature vector and \n",
    "�\n",
    "�\n",
    "y \n",
    "i\n",
    "​\n",
    "  is the corresponding class label (either +1 or -1 for binary classification), the objective of a linear SVM is to find a hyperplane defined by the equation:\n",
    "\n",
    "�\n",
    "⋅\n",
    "�\n",
    "+\n",
    "�\n",
    "=\n",
    "0\n",
    "w⋅x+b=0\n",
    "\n",
    "Here,\n",
    "\n",
    "�\n",
    "w is the weight vector, representing the normal vector to the hyperplane,\n",
    "�\n",
    "x is the input feature vector,\n",
    "�\n",
    "b is the bias term, and\n",
    "⋅\n",
    "⋅ denotes the dot product.\n",
    "The decision function for classifying a new data point \n",
    "�\n",
    "x is given by:\n",
    "\n",
    "�\n",
    "(\n",
    "�\n",
    ")\n",
    "=\n",
    "�\n",
    "⋅\n",
    "�\n",
    "+\n",
    "�\n",
    "f(x)=w⋅x+b\n",
    "\n",
    "The sign of \n",
    "�\n",
    "(\n",
    "�\n",
    ")\n",
    "f(x) determines the predicted class label: if \n",
    "�\n",
    "(\n",
    "�\n",
    ")\n",
    ">\n",
    "0\n",
    "f(x)>0, the predicted class is +1; if \n",
    "�\n",
    "(\n",
    "�\n",
    ")\n",
    "<\n",
    "0\n",
    "f(x)<0, the predicted class is -1.\n",
    "\n",
    "The goal during training is to find the values of \n",
    "�\n",
    "w and \n",
    "�\n",
    "b that maximize the margin between the classes while minimizing the classification error. This is typically formulated as an optimization problem that involves minimizing \n",
    "∥\n",
    "�\n",
    "∥\n",
    "∥w∥ subject to the constraint that \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d57d35b-5a95-4874-a22a-70cd6e0742a8",
   "metadata": {},
   "source": [
    "Q2. What is the objective function of a linear SVM?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "788542bb-7fe6-460e-8a39-d07c9d66fe25",
   "metadata": {},
   "source": [
    "The objective function of a linear Support Vector Machine (SVM) is typically formulated as an optimization problem that aims to find the parameters (\n",
    "�\n",
    "w and \n",
    "�\n",
    "b) of the hyperplane that maximizes the margin between the classes while minimizing classification errors. The standard formulation involves minimizing the norm of the weight vector (\n",
    "∥\n",
    "�\n",
    "∥\n",
    "∥w∥) subject to the constraint that all training examples are correctly classified within a certain margin.\n",
    "\n",
    "The objective function can be written as follows:\n",
    "\n",
    "min\n",
    "⁡\n",
    "�\n",
    ",\n",
    "�\n",
    "1\n",
    "2\n",
    "∥\n",
    "�\n",
    "∥\n",
    "2\n",
    "min \n",
    "w,b\n",
    "​\n",
    "  \n",
    "2\n",
    "1\n",
    "​\n",
    " ∥w∥ \n",
    "2\n",
    " \n",
    "\n",
    "subject to the constraints:\n",
    "\n",
    "�\n",
    "�\n",
    "(\n",
    "�\n",
    "⋅\n",
    "�\n",
    "�\n",
    "+\n",
    "�\n",
    ")\n",
    "≥\n",
    "1\n",
    " for all \n",
    "�\n",
    "y \n",
    "i\n",
    "​\n",
    " (w⋅x \n",
    "i\n",
    "​\n",
    " +b)≥1 for all i\n",
    "\n",
    "Here:\n",
    "\n",
    "∥\n",
    "�\n",
    "∥\n",
    "∥w∥ is the Euclidean norm (magnitude) of the weight vector,\n",
    "�\n",
    "�\n",
    "x \n",
    "i\n",
    "​\n",
    "  represents the feature vectors of the training data,\n",
    "�\n",
    "�\n",
    "y \n",
    "i\n",
    "​\n",
    "  is the corresponding class label (+1 or -1),\n",
    "�\n",
    "b is the bias term, and\n",
    "The constraint \n",
    "�\n",
    "�\n",
    "(\n",
    "�\n",
    "⋅\n",
    "�\n",
    "�\n",
    "+\n",
    "�\n",
    ")\n",
    "≥\n",
    "1\n",
    "y \n",
    "i\n",
    "​\n",
    " (w⋅x \n",
    "i\n",
    "​\n",
    " +b)≥1 ensures that each training example is correctly classified and lies outside a margin defined by the hyperplane.\n",
    "The objective function seeks to find a balance between maximizing the margin (minimizing \n",
    "∥\n",
    "�\n",
    "∥\n",
    "∥w∥) and ensuring correct classification. This optimization problem is typically solved using techniques such as quadratic programming or gradient descent. The resulting \n",
    "�\n",
    "w and \n",
    "�\n",
    "b define the hyperplane that best separates the classes in the input space.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ebc3571-504f-4290-a153-ecf84ab2b269",
   "metadata": {},
   "source": [
    "Q3. What is the kernel trick in SVM?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cae4d0c3-8799-4c13-b113-4964b36bc09f",
   "metadata": {},
   "source": [
    "The kernel trick is a technique used in Support Vector Machines (SVMs) to handle non-linear decision boundaries by implicitly mapping the input data into a higher-dimensional feature space. SVMs, in their basic form, are linear classifiers, meaning they find a hyperplane to separate classes. However, some datasets may not be linearly separable in the original feature space.\n",
    "\n",
    "The kernel trick involves the use of a kernel function, denoted as \n",
    "�\n",
    "(\n",
    "�\n",
    "�\n",
    ",\n",
    "�\n",
    "�\n",
    ")\n",
    "K(x \n",
    "i\n",
    "​\n",
    " ,x \n",
    "j\n",
    "​\n",
    " ), which computes the inner product of the data points in the higher-dimensional space without explicitly transforming them. In other words, instead of explicitly calculating the coordinates of each data point in the higher-dimensional space, the kernel function computes the dot product directly.\n",
    "\n",
    "The decision function of a SVM with a kernel can be expressed as:\n",
    "\n",
    "�\n",
    "(\n",
    "�\n",
    ")\n",
    "=\n",
    "∑\n",
    "�\n",
    "=\n",
    "1\n",
    "�\n",
    "�\n",
    "�\n",
    "�\n",
    "�\n",
    "�\n",
    "(\n",
    "�\n",
    "�\n",
    ",\n",
    "�\n",
    ")\n",
    "+\n",
    "�\n",
    "f(x)=∑ \n",
    "i=1\n",
    "N\n",
    "​\n",
    " α \n",
    "i\n",
    "​\n",
    " y \n",
    "i\n",
    "​\n",
    " K(x \n",
    "i\n",
    "​\n",
    " ,x)+b\n",
    "\n",
    "Here:\n",
    "\n",
    "�\n",
    "N is the number of support vectors,\n",
    "�\n",
    "�\n",
    "α \n",
    "i\n",
    "​\n",
    "  are the Lagrange multipliers obtained during the training process,\n",
    "�\n",
    "�\n",
    "y \n",
    "i\n",
    "​\n",
    "  are the class labels,\n",
    "�\n",
    "�\n",
    "x \n",
    "i\n",
    "​\n",
    "  are the support vectors,\n",
    "�\n",
    "x is the input data,\n",
    "�\n",
    "(\n",
    "�\n",
    "�\n",
    ",\n",
    "�\n",
    ")\n",
    "K(x \n",
    "i\n",
    "​\n",
    " ,x) is the kernel function.\n",
    "Commonly used kernel functions include:\n",
    "\n",
    "Linear kernel: \n",
    "�\n",
    "(\n",
    "�\n",
    "�\n",
    ",\n",
    "�\n",
    "�\n",
    ")\n",
    "=\n",
    "�\n",
    "�\n",
    "⋅\n",
    "�\n",
    "�\n",
    "K(x \n",
    "i\n",
    "​\n",
    " ,x \n",
    "j\n",
    "​\n",
    " )=x \n",
    "i\n",
    "​\n",
    " ⋅x \n",
    "j\n",
    "​\n",
    " \n",
    "Polynomial kernel: \n",
    "�\n",
    "(\n",
    "�\n",
    "�\n",
    ",\n",
    "�\n",
    "�\n",
    ")\n",
    "=\n",
    "(\n",
    "�\n",
    "�\n",
    "⋅\n",
    "�\n",
    "�\n",
    "+\n",
    "�\n",
    ")\n",
    "�\n",
    "K(x \n",
    "i\n",
    "​\n",
    " ,x \n",
    "j\n",
    "​\n",
    " )=(x \n",
    "i\n",
    "​\n",
    " ⋅x \n",
    "j\n",
    "​\n",
    " +c) \n",
    "d\n",
    " \n",
    "Radial basis function (RBF) or Gaussian kernel: \n",
    "�\n",
    "(\n",
    "�\n",
    "�\n",
    ",\n",
    "�\n",
    "�\n",
    ")\n",
    "=\n",
    "exp\n",
    "⁡\n",
    "(\n",
    "−\n",
    "∥\n",
    "�\n",
    "�\n",
    "−\n",
    "�\n",
    "�\n",
    "∥\n",
    "2\n",
    "2\n",
    "�\n",
    "2\n",
    ")\n",
    "K(x \n",
    "i\n",
    "​\n",
    " ,x \n",
    "j\n",
    "​\n",
    " )=exp(− \n",
    "2σ \n",
    "2\n",
    " \n",
    "∥x \n",
    "i\n",
    "​\n",
    " −x \n",
    "j\n",
    "​\n",
    " ∥ \n",
    "2\n",
    " \n",
    "​\n",
    " )\n",
    "Sigmoid kernel: \n",
    "�\n",
    "(\n",
    "�\n",
    "�\n",
    ",\n",
    "�\n",
    "�\n",
    ")\n",
    "=\n",
    "tanh\n",
    "⁡\n",
    "(\n",
    "�\n",
    "�\n",
    "�\n",
    "⋅\n",
    "�\n",
    "�\n",
    "+\n",
    "�\n",
    ")\n",
    "K(x \n",
    "i\n",
    "​\n",
    " ,x \n",
    "j\n",
    "​\n",
    " )=tanh(αx \n",
    "i\n",
    "​\n",
    " ⋅x \n",
    "j\n",
    "​\n",
    " +c)\n",
    "The kernel trick allows SVMs to effectively learn non-linear decision boundaries by operating in a higher-dimensional space without explicitly computing the transformation. This makes SVMs with kernel functions versatile and capable of handling complex relationships in the data.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dd5186c-4841-4c55-9489-46f4efb6ecea",
   "metadata": {},
   "source": [
    "Q4. What is the role of support vectors in SVM Explain with example\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf414f7f-7085-4b8c-b947-ae2131a76c3d",
   "metadata": {},
   "source": [
    "Support vectors play a crucial role in Support Vector Machines (SVMs) and are the key elements that define the decision boundary or hyperplane. In SVM, the objective is to find the hyperplane that best separates the data into different classes while maximizing the margin. The support vectors are the data points that lie closest to the hyperplane and influence its position.\n",
    "\n",
    "Here's how support vectors and the hyperplane work in SVM:\n",
    "\n",
    "Identifying Support Vectors:\n",
    "During the training process, the SVM algorithm identifies a subset of data points as support vectors. These are the points that either lie on the margin or violate the margin constraint (lying on the wrong side of the decision boundary).\n",
    "\n",
    "Defining the Hyperplane:\n",
    "The hyperplane is determined by the support vectors. It is positioned to maximize the margin between the classes, and its orientation is influenced by the support vectors. The decision function of the SVM is defined in terms of these support vectors.\n",
    "\n",
    "Computing the Decision Function:\n",
    "The decision function for a linear SVM is given by:\n",
    "�\n",
    "(\n",
    "�\n",
    ")\n",
    "=\n",
    "∑\n",
    "�\n",
    "=\n",
    "1\n",
    "�\n",
    "�\n",
    "�\n",
    "�\n",
    "�\n",
    "�\n",
    "�\n",
    "⋅\n",
    "�\n",
    "+\n",
    "�\n",
    "f(x)=∑ \n",
    "i=1\n",
    "N\n",
    "​\n",
    " α \n",
    "i\n",
    "​\n",
    " y \n",
    "i\n",
    "​\n",
    " x \n",
    "i\n",
    "​\n",
    " ⋅x+b\n",
    "Here, \n",
    "�\n",
    "�\n",
    "α \n",
    "i\n",
    "​\n",
    "  are the Lagrange multipliers obtained during training, \n",
    "�\n",
    "�\n",
    "y \n",
    "i\n",
    "​\n",
    "  are the class labels, \n",
    "�\n",
    "�\n",
    "x \n",
    "i\n",
    "​\n",
    "  are the support vectors, and \n",
    "�\n",
    "x is the input data.\n",
    "\n",
    "Determining Class Labels:\n",
    "The sign of \n",
    "�\n",
    "(\n",
    "�\n",
    ")\n",
    "f(x) determines the predicted class. If \n",
    "�\n",
    "(\n",
    "�\n",
    ")\n",
    ">\n",
    "0\n",
    "f(x)>0, the input \n",
    "�\n",
    "x is classified as one class; if \n",
    "�\n",
    "(\n",
    "�\n",
    ")\n",
    "<\n",
    "0\n",
    "f(x)<0, it is classified as the other class.\n",
    "\n",
    "Example:\n",
    "Consider a binary classification problem with two features (\n",
    "�\n",
    "1\n",
    "x \n",
    "1\n",
    "​\n",
    "  and \n",
    "�\n",
    "2\n",
    "x \n",
    "2\n",
    "​\n",
    " ). The SVM aims to separate two classes, +1 and -1. The support vectors (circled in the figure below) are the data points that lie closest to the decision boundary (hyperplane).\n",
    "\n",
    "\n",
    "In this example, the support vectors are crucial because they determine the position and orientation of the hyperplane. Changing or removing a non-support vector would not affect the hyperplane. However, removing a support vector could lead to a different hyperplane, potentially impacting the SVM's performance.\n",
    "\n",
    "The margin is maximized by considering the support vectors, and the SVM is inherently robust because it focuses on the most relevant data points during training.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7aa591b-3e5f-4ad9-9772-2a8910f158d2",
   "metadata": {},
   "source": [
    "Q5. Illustrate with examples and graphs of Hyperplane, Marginal plane, Soft margin and Hard margin in\n",
    "SVM?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cfebfc4-e51c-4eb2-990f-abadc0d5cb57",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
